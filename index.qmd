---
title: "Métodos computacionales para las ciencias sociales"
subtitle: "Procesamiento de texto III"
format: 
    revealjs:
      auto-stretch: false
      scrollable: true
      link-external-newwindow: true
css: style.css
editor: source
execute:
  echo: false
---

```{r, echo=FALSE}
source("code/helpers.R")
library(kableExtra)
library(reticulate)
library(tidyverse)

use_virtualenv("taller-nlp") # activar un ambiente espećifico

```



## Contenidos de la clase

- Word embeddings (spacy)
- Combinando R y python: reticulate
- Análisis de sentimiento (spacy)
 


## Recordando...

Sabemos que es conveniente convertir los textos en vectores  

. . .

Ya revisamos cómo hacerlo mediante la estrategia de bolsa de palabras (*bag of words*)

. . .

Hoy veremos una estrategia llamada *word embeddings* 


## Motivación

*lindo perro*

*bonito can*

. . .

¿Cuál sería el valor de similitud coseno en un enfoque TFIDF?

. . .


<img src="imagenes/cosine.jpg" width="800" />

  
. . .

**Necesitamos algo que capture el significado de las palabras**


##  ¿Qué es *word embeddings*?

Cualquier mecanismo que permita convertir palabras en vectores

. . .

**Idea clave**: El significado de una palabra está dada por su contexto 

. . .

La palabra **nación** está más cerca semánticamente a **país** que la palabra **hipopótamo** 

. . .

La palabra **animal** está asociada a la palabra **hipopótamo**

. . .

No es tan común que la palabra **nación** esté cerca de **animal**

. . .

Para un humano esto es trivial, pero para un computador es muy difícil

. . .

Podemos entrenar a una red neuronal para que aprenda vectores de palabras  


## Antes, un poco de historia


Todo comenzó con enfoques tipo bolsa de palabras (clase anterior)

- BOW
- DFM
- TF-IDF



## Antes, un poco de historia


Matriz de co-ocurrencia, reducción de dimensionalidad y cadenas de Markov

```{r}
library(quanteda)

tokens_gato <- tokens(c("Mi gato es un tirano. Él es amo 
                  y señor de la casa"))
fcm(tokens_gato, context = "window", window = 2)

```


## Artículo clave (Mikolov et al, 2013)

Efficient Estimation of Word Representations in Vector Space

<img src="imagenes/mikolov.png" width="700" />


## Algunas intuiciones

*La programación me ha permitido ahorrar muchas horas de trabajo*

. . .

window: **2**

word: **ahorrar**

**CBOW**

ha permitido ______ muchas horas

. . .

**Skip-gram**


____ _____ ahorrar _____ ______     



## A más bajo nivel

La capa de entrada corresponde a vectores one-hot

```{python}
print([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
print([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
print([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
print([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
```


. . .

La capa de salida es un vector one-hot

```{python}
print([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])
```


. . .

La capa oculta es una representación distribuida de cada palabra

- Muchas veces se usa una capa oculta con 300 neuronas (perceptrones)

## Entrenamiento

Podemos tener muchos pares de palabra-contexto

. . .

- La naranja y la **manzana** son frutas

- La **manzana** es roja

- Ella mordió la **manzana** envenenada que le dio la bruja

- Decidieron morder la **manzana** del conocimiento

- Esa es una **manzana** podrida dentro del grupo

- Me como, al menos, una **manzana** al día

. . .

**Entrenamos una red para que aprenda una representación de manzana**





## Algunos recursos

Para una aproximación más formal, revisen las clases de [Jorge Pérez](https://www.youtube.com/@JorgePerez-pt3qg)

- Algo de álgebra lineal
- Un poco de cálculo (límites, derivadas)


. . .


Para una entrada más intuitiva vean [dot csv](https://www.youtube.com/watch?v=Tg1MjMIVArc)

## Usaremos spacy de python

::: panel-tabset

## spacy

Vamos a cargar vectores de una librería de python llamada [spacy](https://spacy.io/)  

![](imagenes/spacy.png){fig-align="center" width="420px"}

![](imagenes/logo_python.png){fig-align="center" width="130px"}


## opciones


**Opción 1: Usar python nativamente (más fácil)** 


**Opción 2: Correr python desde un entorno R** 

:::



## Opción 1: IDE especializado

![](imagenes/ides.png){fig-align="center" width="800"}



## Opción 2: reticulate

::: panel-tabset

## Instalación


![](imagenes/reticulate.png){fig-align="center" width="460px"}

```{r, eval=FALSE, echo=TRUE}
install.packages("reticulate")
library(reticulate)
```

## Ambiente virtual


Si todo sale bien, esto debería ser suficiente



```{r, eval=FALSE, echo=TRUE}
library(reticulate)
virtualenv_create("taller-nlp", version = "3.10.12" , packages = F) # crear un ambiente virtual
use_virtualenv("taller-nlp") # activar un ambiente espećifico
virtualenv_install("taller-nlp", "numpy") # instalar numpy
virtualenv_install("taller-nlp", "spacy") # instalar spacy
py_run_string("import os; os.system('python -m spacy download es_core_news_lg')") # instalar modelo grande de embeddings
spacy <-  reticulate::import("spacy" ) # cargar spacy en R
nlp =  spacy$load("es_core_news_lg") # cargar modelo pequeño con 96 dimensiones

```

## Posibles problemas

Reticulate no conoce la ruta del ejecutable de Python

Versiones incompatibles de librerías de Python

Otras cosas misteriosas


## Procesar

```{r, echo=FALSE}
poner_nombres <- function(vector) {
  names(vector) <- paste0("dim", 1:length(vector))
  return(vector)
}
```



```{r, echo=TRUE}
library(tidyverse)
use_virtualenv("taller-nlp") # activar un ambiente espećifico
spacy <-  reticulate::import("spacy" ) # cargar spacy en R
nlp =  spacy$load("es_core_news_lg") # cargar modelo pequeño con 96 dimensiones

doc = nlp("limón pera manzana sandía melón rojo azul amarillo verde perro gato ratón tigre elefante")

# Crear vectores para cada una de las palabras
indices <- 0:(length(doc) - 1)
vectores <- map(indices,  ~doc[.x]$vector ) %>% 
  map(poner_nombres) %>% 
  bind_rows()

```


## python

```{python, echo = TRUE, eval = TRUE}
# Cargar modelo
import spacy
import pandas as pd
nlp =  spacy.load("es_core_news_lg")
doc = nlp("limón pera manzana sandía melón rojo azul amarillo verde perro gato ratón tigre elefante")

# Extraer vectores
vectors = []
for tok in doc:
  vectors.append(tok.vector)

# Convertir a dataframe
vectors_df =  pd.DataFrame(vectors)

```





## Resultado

Cada palabra está representada por un vector de 300 dimensiones (elementos)

```{r, echo=TRUE}
dim(vectores)
print(unlist(vectores[1, ]))


```


:::




## Word embeddings


```{r pca, echo=FALSE, fig.height=5, fig.align="center"}

library(stats)
resultado <- prcomp(vectores, scale = TRUE)

df <-  data.frame(dim1 = resultado$x[, 1], dim2 = resultado$x[, 2]) %>% 
  mutate(objeto = c("fruta", "fruta", "fruta", "fruta", "fruta", "color", "color", "color", "color", "animal",
                    "animal", "animal", "animal", "animal" ),
         word = c("limón", "pera", "manzana", "sandía", "melón", "rojo", "azul", "amarillo", "verde", "perro",
                  "gato", "ratón", "tigre", "elefante")
         )
df %>% 
  ggplot(aes(dim1, dim2, color = objeto, label = word)) +
  geom_point() +
  scale_color_manual(values = c("fruta" = "green", "color" = "blue", "animal" = "red")) +
  geom_text() +
  theme_bw()



df_3d <-  data.frame(dim1 = resultado$x[, 1], dim2 = resultado$x[, 2], dim3 = resultado$x[, 3]) %>% 
  mutate(objeto = c("fruta", "fruta", "fruta", "fruta", "fruta", "color", "color", "color", "color", "animal",
                    "animal", "animal", "animal", "animal" ),
         word = c("limón", "pera", "manzana", "sandía", "melón", "rojo", "azul", "amarillo", "verde", "perro",
                  "gato", "ratón", "tigre", "elefante")
         )


```
. . .

```{r, echo=FALSE}
pal <- c("red", "blue", "green")

library(plotly)
fig <- plot_ly(df_3d, x = ~dim1, y = ~dim2, z = ~dim3, color = ~objeto, text = ~word, colors = pal)
fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = 'dim1'),
                     yaxis = list(title = 'dim2'),
                     zaxis = list(title = 'dim3'))) 

fig

```

## Palabras más cercanas


```{python, echo = T}
from scipy.spatial.distance import cosine

def palabra_mas_cercana(vector_objetivo, vocabulario):
    palabra_cercana = None
    menor_distancia = float("inf")
    for palabra in vocabulario:
        vector = nlp.vocab[palabra].vector
        if vector.any() and sum(vector_objetivo == vector) != 300:  # Ignorar palabras sin vector
            distancia = cosine(vector_objetivo, vector)
            if distancia < menor_distancia :
                menor_distancia = distancia
                palabra_cercana = palabra
    return palabra_cercana, menor_distancia

# Vocabulario a comparar (puedes usar una lista específica o todo nlp.vocab)
vocabulario = list(nlp.vocab.strings)


# Elegir una palabra
word = "ropa"
vector =  nlp(word).vector

# Encontrar la palabra más cercana al vector resultante
closest_word, distance = palabra_mas_cercana(vector, vocabulario)
print(f"La palabra más cercana a {word} es {closest_word}, con similitud {distance}")
```


## Comentario spacy

Los embeddings de spacy son muy livianos y fáciles de usar

. . .

No son tan poderosos

. . .

Gensim tiene mejores embeddings (4 GB)



## Paréntesis de Reticulate: más ejemplos

::: panel-tabset

## instalando paquetes

```{r, eval=FALSE, echo=TRUE}
reticulate::virtualenv_install("taller-nlp", "pandas")
```


## dataframe

```{python, echo = T}
import pandas as pd
df_python =  pd.DataFrame({"var1": [1, 2, 3], "var2": [4, 5, 6]}  )
print(df_python)
```

```{r, eval=TRUE, echo=TRUE}
df_r =  py$df_python
print(df_r)



```

## función

```{python, echo = TRUE}
def sumar(x, y):
  return x + y
```

```{r, echo=TRUE}
sumar <-  py$sumar
sumar(3, 5)

```


:::


## Usemos los vectores

Usaremos los vectores de *spacy* para procesar las noticias

. . .

**Plan:** 

1. Separar todas las noticias en oraciones
2. Convertir las oraciones en vectores
3. Buscar algunos tópicos, utilizando similitud coseno


## Aplicando lo aprendido

```{r, echo=TRUE}
library(quanteda)
data <- read_csv("data/data_larazon_publico_v2.csv")

set.seed(123)
corpus <- data %>% 
  select(text = cuerpo) %>% 
  sample_frac(0.2) %>% 
  corpus()

tokens <- corpus %>% 
  tokens()

dfm <- tokens %>% 
  dfm()  


```

## Preprocesamiento

```{r separar textos, eval=T, echo=TRUE}
# Esto es un listado de oraciones
oraciones_editadas <- map(corpus, ~split_and_edit(.x ) )  %>% 
  flatten()  # desanclarar oraciones de la noticia

# Sin editar, para usar cuando miremos los datos
oraciones <- map(corpus, ~split_text(.x)) %>% 
  flatten()

```

```{r mostrar oraciones, echo=FALSE}
data.frame(oraciones = oraciones_editadas[1:2] %>% unlist()) %>% 
  kbl() %>% 
  kable_styling()

```


## Usando los vectores de spacy

Debido a la remoción de palabras, pueden haber quedado algunas oraciones vacías

```{r editar datos, echo=TRUE}
# Eliminamos las oraciones sin tokens
texto_vacio <- oraciones_editadas == ""
oraciones_editadas <- oraciones_editadas[!texto_vacio]
oraciones <- oraciones[!texto_vacio]

```

. . .


Haremos una muestra de 50.000 oraciones

```{r muestra, echo=TRUE}
set.seed(123)
# Números aleatorios entre 1 y 311176
vector_muestra <-  sample(x = 1:length(oraciones_editadas), 50000, replace = FALSE)

# Hacemos la selección en las oraciones editadas y no editadas
oraciones_muestra <- oraciones_editadas[vector_muestra]
oraciones_muestra_originales <- oraciones[vector_muestra]

```

## Pasar de texto a vector 

::: panel-tabset

## texto

```{r, echo=TRUE}
oraciones_muestra[[1]]

```


Esto es lo que sabemos hasta ahora

```{r, echo=TRUE}
doc = nlp(oraciones_muestra[[1]])
vector_psoe <-  doc[[1]]$vector

```



## pregunta

Tenemos un vector para cada palabra

¿Cómo convertir un texto en vector?

![](https://media.giphy.com/media/UP9ItQNj52DsM3e29m/giphy.gif){fig-align="center" width="200px"}


Una manera es calcular la media de cada una de las 300 dimensiones

## media

```{r, echo=TRUE}
texto <-  oraciones_muestra[[1]]
var_names <- paste0("dim", 1:300)
doc = nlp(texto)  
indices <- 0:(length(doc) - 1)
  
vectores <- map(indices,  ~doc[.x]$vector ) %>% 
  discard(function(x) sum(x) == 0 ) %>% # sacar las palabras no incluidas en el vocabulario
  map(set_names, var_names) %>% 
  bind_rows()
  
representacion <-  map(vectores, mean)

```

Encapsulamos todo en una función llamada `create_representation`

:::

## Procesamiento

La función `create_representation` convierte un texto en un vector 

. . .

Devuelve una lista con 3 elementos

`return(list(representacion, texto, length(indices)) )`


. . .

**Advertencia**: Este proceso puede tomar varios minutos

```{r muestreo textos, eval=FALSE, echo=TRUE}
representations <- map(oraciones_muestra, create_representation) 
saveRDS(representations, "data/vector_representation_sample.rds")
```


## Cargar información


```{r, echo=TRUE}
representations <- readRDS("data/vector_representation_sample.rds")
print(representations[[1]][1] %>% unlist() %>% unname() %>% as.numeric())
print(representations[[1]][2])

```


## Un poco más de procesamiento

```{r cargar representaciones, echo=TRUE}

# Agregar oraciones originales a la lista
representations <- map2(oraciones_muestra_originales, representations, ~append(.y, .x))

# Eliminar textos sin representación y con menos de 4 palabras
representations2 <- representations %>%
  keep(~!is_empty(.x[[1]])) %>% # sacar las oraciones sin representación  
  keep(~.x[3] > 4) # sacar textos con pocas palabras

# Guardamos los vectores
vectores <- representations2 %>% 
  map(1) %>% 
  map(~unlist(unname(.x)))  

# Guardamos las oraciones editadas
textos <- representations2 %>% 
  map(2) 

# Guardamos las oraciones editadas
textos_originales <- representations2 %>% 
  map(4) 


```


## Buscando tópicos


::: panel-tabset

## función

```{r, echo=TRUE, eval=FALSE}
function(vectores, texto, n = 3, lista_textos ) {
  vector <-  create_representation(texto) # ya la conocemos
  vector <- vector[[1]] %>% unname() %>% unlist() # simplificar lista
  similitud <-  map_dbl(vectores, ~coop::cosine(vector, .x ) ) # comparar cada vector contra todos los demás
  top <- order(similitud, decreasing=T)[1:n] # ordenar de mayor a menor similitud
  return(lista_textos[top])
}
```

## tópicos

```{r ejemplo buscar concepto , eval=FALSE, echo=TRUE}
encontrar_mas_parecidos(vectores, "elecciones congreso nacional", 5, lista_textos =  textos_originales)
encontrar_mas_parecidos(vectores, "crecimiento económico", 5, lista_textos =  textos_originales)
encontrar_mas_parecidos(vectores, "inmigrantes musulmanes", 5, lista_textos =  textos_originales)
encontrar_mas_parecidos(vectores, "desigualdad económica", 5, lista_textos =  textos_originales)

encontrar_mas_parecidos(vectores, "ministerio sanidad", lista_textos =  textos_originales)
encontrar_mas_parecidos(vectores, "inflación", 5, lista_textos =  textos_originales)
encontrar_mas_parecidos(vectores, "terrorismo", 5, lista_textos =  textos_originales)
encontrar_mas_parecidos(vectores, "violencia género", 5, lista_textos =  textos_originales)



```


:::

## Resultados

::: panel-tabset


## elecciones
```{r congreso, echo=FALSE}
x <- encontrar_mas_parecidos(vectores, "elecciones congreso nacional", 5, lista_textos =  textos_originales)
crear_tabla(x)
```


## crecimiento

```{r crecimiento, echo=FALSE}
x <- encontrar_mas_parecidos(vectores, "crecimiento económico", 5, lista_textos =  textos_originales)
crear_tabla(x)
```


## inmigrantes
```{r inmigrantes, echo=FALSE}
x <- encontrar_mas_parecidos(vectores, "inmigrantes musulmanes", 5, lista_textos =  textos_originales)
crear_tabla(x)
```

## desigualdad
```{r desigualdad, echo=FALSE}
x <- encontrar_mas_parecidos(vectores, "desigualdad económica", 5, lista_textos =  textos_originales)
crear_tabla(x)
```

## sanidad
```{r ministerio, echo=FALSE}
x <- encontrar_mas_parecidos(vectores, "ministerio sanidad",  5, lista_textos =  textos_originales)
crear_tabla(x)
```

## inflación
```{r inflacion, echo=FALSE}
x <- encontrar_mas_parecidos(vectores, "inflación", 5, lista_textos =  textos_originales )
crear_tabla(x)
```

## terrorismo
```{r terrorismo, echo=FALSE}
x <- encontrar_mas_parecidos(vectores, "terrorismo", 5, lista_textos =  textos_originales)
crear_tabla(x)
```

## género
```{r violencia genero, echo=FALSE}
x <- encontrar_mas_parecidos(vectores, "violencia género", 5, lista_textos =  textos_originales)
crear_tabla(x)
```

:::

## Nuestros propios embeddings


Los modelos de spacy han sido entrenados de manera muy general

. . .

A veces, podemos tener corpus muy específicos (temas científicos, dialectos, etc.)  

. . .

Podemos entrenar nuestros embeddings dentro de R

**warning**: esto puede tardar bastante 

. . .

```{r, eval=FALSE, echo=TRUE}
library(word2vec)

x <- data$cuerpo # noticias

# Generar embeddings de 300 dimensiones
model <- word2vec(x = x, dim = 300, iter = 20, min_count = 5, threads = 16)

# Guardamos el modelo 
write.word2vec(model, "data/news_vectors_300.bin")


# Crear una matriz
embedding <- as.matrix(model)

# Buscar palabras más cercanas 
lookslike <- predict(model, c("democracia", "violencia", "presidente", "dictadura", "rojo", "rey"), 
                     type = "nearest", top_n = 5)

```


```{r cargar modelo, echo=FALSE}
model <- word2vec::read.word2vec("data/news_vectors_300.bin")

cercanos <- predict(model, c("inmigración", "democracia", "violencia", "presidente", "dictadura", "rey" ), 
                     type = "nearest", top_n = 5)

# Crear una matriz
embedding <- as.matrix(model)

cercanos %>% 
  bind_rows() %>% 
  kbl() %>%
  kable_styling(font_size = 18)

```

# Análisis de sentimiento {.center background-color="aquamarine"}



## Análisis de sentimiento: breve historia

![](imagenes/neurona.jpg){fig-align="center" width="800"}





## Análisis de sentimiento: breve historia


::: panel-tabset

## Perceptrón

![](imagenes/perceptron.png){fig-align="center" width="500"}
$$
\text{output} =
\begin{cases}
0 & \text{si } \sum_j w_j x_j \leq \text{umbral} \\
1 & \text{si } \sum_j w_j x_j > \text{umbral}
\end{cases}
\tag{1}
$$


## Bias


$$
\text{output} =
\begin{cases}
0 & \text{si } \mathbf{w} \cdot \mathbf{x} + b \leq 0 \\
1 & \text{si } \mathbf{w} \cdot \mathbf{x} + b > 0
\end{cases}
\tag{2}
$$

**Bias**: Qué tan fácil es que el output sea 1. 

## step function

![](imagenes/step-function.png){fig-align="center" width="500"}

## Sigmoid

![](imagenes/sigmoid-function.png){fig-align="center" width="500"}

## Funciones activación

![](imagenes/activation-functions.png){fig-align="center" width="800"}

:::


## Análisis de sentimiento: breve historia

Red *feed fordward*

![](imagenes/feed-forward.png){fig-align="center" width="800"}


## Análisis de sentimiento: breve historia

![](imagenes/neurona-ejemplo.png){fig-align="center" width="900"}

## Análisis de sentimiento: breve historia

Las redes feed forward son extremadamente poderosas 

. . .

**Teorema de aproximación universal**:

- Una red de una capa oculta puede aprender cualquier función continua

. . .

- Podemos aproximarnos cuánto queramos 

. . .

$\varepsilon$: precisión deseada

Para una red con *output* $g(x)$, siempre se puede satisfacer la condición
$$
|g(x) - f(x)| < \varepsilon
$$


## Análisis de sentimiento: breve historia

- Las redes feed forward son extraordinarias, pero tienen defectos para procesamiento de texto

. . .

RNN: Recurrent Neural Network

![](imagenes/rnn.png){fig-align="center" width="800"}



**Problema de memoria de largo plazo**


##  Análisis de sentimiento: breve historia


*Mi tía tiene una casa cerca del barrio Bellavista. Ella siempre se queja del ruido, lo que la ha convertido en la señora gruñona de la cuadra*

. . .

Término técnico: desvanecimiento del gradiente

. . .

**Innovaciones con recurrencia**

- GRU: Gated Recurrent Unit 
- LSTM: Long Short Term Memory

. . .

Embeddings (Mikolov et al, 2013)

. . .

**Mecanismo de atención (utiliza *embeddings*)**

- Puedo calcular cuál es la cercanía entre palabras y considerar eso en el entrenamiento  

. . .

*Attention is all you need* (2017)

- Eliminan la recurrencia y solo usan mecanismo de atención
- Esto se puede paralelizar
- Aumenta la cantidad de datos


. . .

**Grandes modelos de lenguaje (>2018)**

- GPT1 - 2 - 3 - 4
- Gemini
- Llama



## Análisis de sentimiento

Retomemos nuestros textos sobre la ETA 

Trataremos de ver si los extractos tienen un "sentimiento" positivo o negativo 

Usaremos un modelo disponible en [huggingface](https://huggingface.co/pysentimiento/robertuito-sentiment-analysis)

Empresa que pone a disposición transformers


![](imagenes/huggingface.png){fig-align="center" width="420px"}


## Huggingface


```{r, eval=FALSE, echo=TRUE}
reticulate::py_install("transformers", pip = TRUE) # librería para los modelos
reticulate::py_install(c("torch", "sentencepiece"), pip = TRUE) # backend para el manejo de matrices
```


Cargamos la librería y un modelo entrenado con datos en español

```{r, echo=TRUE}
transformers <- reticulate::import("transformers")
classifier <- transformers$pipeline(task = "text-classification",
                                    model = "pysentimiento/robertuito-sentiment-analysis" )

```

## Ejemplo básico

```{r, echo=TRUE}

text <- c("Es lo mejor que me ha pasado en el último tiempo")
classifier(text)

text <- c("Es el peor producto que he comprado en mi vida")
classifier(text)

```

## Menciones a ETA con quanteda


```{r, echo=TRUE}
menciones_eta <- tokens %>% 
  kwic( pattern = "eta",  window = 7)  

```

```{r}
menciones_eta %>% 
  DT::datatable(rownames = F, 
                options = list(
                  pageLength = 3,
                  dom = "rtip",
                  headerCallback = DT::JS(
                    "function(thead) {",
                    "  $(thead).css('font-size', '0.8em');",
                    "}"
                  ))
                )%>% 
  DT::formatStyle(columns = 1:ncol(menciones_eta), fontSize = '70%', backgroundSize = '30%')

```

## Predicciones con huggingface

```{r, echo=TRUE}

# Armar dataframe con los primeros 10 extractos
data <- data.frame(pre  = menciones_eta$pre, post = menciones_eta$post ) %>% 
  slice(1:10) %>% 
  mutate(text = paste(pre, post)) %>% 
  select(text)

# Predecir sentimiento para cada uno de los textos
out <-  map(data$text, classifier)

# Extraer la etiqueta de cada predicción
labels <- flatten(out) %>%
  map_chr(1)

# Extraer probabilidad de cada predicción
prob <- flatten(out) %>%
  map_dbl(2)

# Añadir columnas a data frame
data <- data %>% 
  mutate(sentiment = labels,
         prob = prob)

```




## Predicciones

```{r, echo=FALSE}
data %>% 
  kbl() %>% 
  kable_styling(font_size = 20)
  
```


## Ejemplo final: datamedios

**Queremos ver cómo cambian los sentimientos de los títulos de las noticias a lo largo del tiempo**

. . .

Cargamos el archivo

```{r}
directorio <- "/home/klaus/proyectos-personales/analizar_noticias/articulo/data/"
```

```{r cargar noticias, echo=TRUE}
library(arrow)
library(dplyr)

noticias <- read_parquet( paste0(directorio, "noticias_respaldo.parquet"), as_data_frame = F )

```

## Ejemplo final: datamedios

Miremos rápidamente qué contiene

```{r, echo=TRUE}
rows <- noticias |> 
  summarize(total = n()) |> 
  collect()

print(sprintf("El archivo contiene %s filas", rows))
print(names(noticias))
```

. . .

```{r, echo=TRUE}
noticias |> 
  head(10) |> 
  collect() |> 
  DT::datatable()
```


## Ejemplo final: datamedios

Haremos la predicción de sentimiento para los primeros 10 títulos

```{r, echo=TRUE}
first_rows = noticias |> 
  head(10) |> 
  collect()

# Predicciones
out <-  map(first_rows$titulo, classifier)

# Extraer la etiqueta de cada predicción
labels <- flatten(out) %>%
  map_chr(1)

```


## Ejemplo final: datamedios

Imaginemos que ya hicimos la predicción para todos los títulos a nivel de mes

```{r, echo=TRUE}
library(readr)
sentimientos <- read_csv("data/datamedios_sentimiento.csv")
sentimientos |> 
  select(medio, periodo, pct_POS, pct_NEG, pct_NEU) |> 
  head()

```


## Ejemplo final: datamedios

::: panel-tabset

## Procesamiento

Pequeño procesamiento

```{r, echo=TRUE}
data_plot <- sentimientos |> 
  select(medio, periodo, pct_POS, pct_NEG, pct_NEU) |> 
  tidyr::pivot_longer(cols = c("pct_POS", "pct_NEG", "pct_NEU" ), 
                      names_to = "sentimiento",
                    values_to = "porcentaje" ) |> 
  filter(periodo >= "2010-01" ) |> 
  mutate(periodo = as.Date(paste0(periodo, "-01"))) 

```


## gráfico

```{r, echo=TRUE}
library(ggplot2)
library(plotly)

p <- data_plot |> 
  ggplot(aes(x = periodo, y = porcentaje, group = sentimiento, color = sentimiento)) +
  geom_line() +
  facet_wrap(~medio, ncol = 1) +
  scale_x_date(date_breaks = "3 months", date_labels = "%Y-%m") +
  theme(axis.text.x = element_text(angle = 90))

ggplotly(p)

```

:::




## Algunos comentarios

**En la clase revisamos:**

- *Word Embeddings* (spacy)
- Clasificador de sentimientos (transformers)

. . .

`R` cuenta con varias herramientas para trabajar con datos de texto

. . .

* Las herramientas más populares en la actualidad están disponibles en *Python*
  + spacy
  + transformers
  + gensim
  + otras

. . .

- Podemos articular ambos lenguajes a través de `reticulate` 

* A veces, lo más sencillo es separar las tareas entre R y python
  + Creamos el dataset con R y guardamos un archivo (csv o parquet) 
  + Abrimos el archivo con Python
  + Ejecutamos el modelo de Huggingface


# Métodos computacionales para las ciencias sociales {.center background-color="aquamarine"}
